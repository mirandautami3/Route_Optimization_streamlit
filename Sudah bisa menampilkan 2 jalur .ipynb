{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import osmnx as ox\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import folium\n",
    "from matplotlib import cm\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define depot and customer locations\n",
    "# locations = [\n",
    "#     (-7.266735,112.736115),  # Depot\n",
    "#     (-7.291418,112.677844)   # Customer 1\n",
    "# ]\n",
    "# depot_location = locations[0]\n",
    "\n",
    "# # Fetch road network with extended distance\n",
    "# G = ox.graph_from_point(depot_location, dist=15000, network_type='drive')\n",
    "\n",
    "# # Function to validate nearest nodes and map locations to the road network\n",
    "# def map_to_nearest_nodes(locations, graph, distance_threshold=2000):\n",
    "#     valid_nodes = []\n",
    "#     valid_demands = []\n",
    "#     for idx, (lat, lng) in enumerate(locations):\n",
    "#         nearest_node = ox.nearest_nodes(graph, lng, lat)\n",
    "#         nearest_point = (graph.nodes[nearest_node]['y'], graph.nodes[nearest_node]['x'])\n",
    "#         dist_to_nearest = ox.distance.great_circle(lat, lng, nearest_point[0], nearest_point[1])\n",
    "#         print(f\"Location {idx}: Distance to nearest node = {dist_to_nearest:.2f} meters\")\n",
    "#         if dist_to_nearest > distance_threshold:  # Skip locations too far from the network\n",
    "#             print(f\"Warning: Location {idx} is far from the road network! Skipping this location.\")\n",
    "#             continue\n",
    "#         valid_nodes.append(nearest_node)\n",
    "#         if idx > 0:  # Exclude depot from demands\n",
    "#             valid_demands.append(10)\n",
    "#     return valid_nodes, valid_demands\n",
    "\n",
    "# # Map locations to nearest nodes\n",
    "# valid_nodes, valid_demands = map_to_nearest_nodes(locations, G)\n",
    "\n",
    "# # Convert graph to undirected for connectivity check\n",
    "# G_undirected = G.to_undirected()\n",
    "\n",
    "# # Check connectivity of the graph\n",
    "# if not nx.is_connected(G_undirected):\n",
    "#     print(\"Warning: The road network graph is not fully connected. Consider increasing the distance or changing locations.\")\n",
    "# else:\n",
    "#     print(\"The road network graph is fully connected.\")\n",
    "\n",
    "# # Build distance matrix for valid nodes\n",
    "# def build_distance_matrix(valid_nodes, graph):\n",
    "#     distance_matrix = np.zeros((len(valid_nodes), len(valid_nodes)))\n",
    "#     paths = {}\n",
    "#     for i, start_node in enumerate(valid_nodes):\n",
    "#         for j, end_node in enumerate(valid_nodes):\n",
    "#             if i != j:\n",
    "#                 try:\n",
    "#                     path = nx.shortest_path(graph, start_node, end_node, weight='length')\n",
    "#                     distance = nx.shortest_path_length(graph, start_node, end_node, weight='length')\n",
    "#                     distance_matrix[i][j] = distance\n",
    "#                     paths[(i, j)] = path\n",
    "#                 except nx.NetworkXNoPath:\n",
    "#                     print(f\"Warning: No path between nodes {start_node} and {end_node}\")\n",
    "#                     distance_matrix[i][j] = float('inf')\n",
    "#                     paths[(i, j)] = []\n",
    "#     return distance_matrix, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/osmnx/graph.py:191: FutureWarning: The expected order of coordinates in `bbox` will change in the v2.0.0 release to `(left, bottom, right, top)`.\n",
      "  G = graph_from_bbox(\n"
     ]
    }
   ],
   "source": [
    "# Dataset baru: Jalur dari Stasiun Gubeng ke tunjungan plaza\n",
    "routes_data = [\n",
    "    {\n",
    "        \"Condition\": \"lancar\",\n",
    "        \"Route\": [(-7.257472, 112.752088), (-7.261500, 112.744000), (-7.266735, 112.736115)],\n",
    "        \"Time\": 10,  # in minutes\n",
    "        \"Distance\": 3.5  # in km\n",
    "    },\n",
    "    {\n",
    "        \"Condition\": \"macet\",\n",
    "        \"Route\": [(-7.257472, 112.752088), (-7.263000, 112.740000), (-7.266735, 112.736115)],\n",
    "        \"Time\": 20,  # in minutes\n",
    "        \"Distance\": 4.0  # in km\n",
    "    }\n",
    "]\n",
    "\n",
    "# Fetch road network with extended distance\n",
    "start_location = routes_data[0][\"Route\"][0]\n",
    "G = ox.graph_from_point(start_location, dist=15000, network_type='drive')\n",
    "\n",
    "# Map routes to nearest nodes and find actual paths on the road network\n",
    "def map_and_find_paths(routes, graph):\n",
    "    mapped_routes = []\n",
    "    for route in routes:\n",
    "        nodes = []\n",
    "        for lat, lng in route[\"Route\"]:\n",
    "            nearest_node = ox.nearest_nodes(graph, lng, lat)\n",
    "            nodes.append(nearest_node)\n",
    "\n",
    "        # Find shortest path between consecutive nodes\n",
    "        path_coords = []\n",
    "        for i in range(len(nodes) - 1):\n",
    "            try:\n",
    "                # Get the shortest path (list of nodes)\n",
    "                path = nx.shortest_path(graph, nodes[i], nodes[i + 1], weight=\"length\")\n",
    "                # Convert path nodes to coordinates\n",
    "                path_coords.extend([(graph.nodes[node]['y'], graph.nodes[node]['x']) for node in path])\n",
    "            except nx.NetworkXNoPath:\n",
    "                print(f\"Warning: No path between {nodes[i]} and {nodes[i + 1]} in {route['Condition']} route.\")\n",
    "        \n",
    "        mapped_routes.append({\n",
    "            \"Condition\": route[\"Condition\"],\n",
    "            \"Path\": path_coords,\n",
    "            \"Time\": route[\"Time\"],\n",
    "            \"Distance\": route[\"Distance\"]\n",
    "        })\n",
    "    return mapped_routes\n",
    "\n",
    "# Map and calculate paths for all routes\n",
    "mapped_routes_with_paths = map_and_find_paths(routes_data, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define DQN model\n",
    "# class DQN(tf.keras.Model):\n",
    "#     def __init__(self, num_actions):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.fc1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "#         self.fc2 = tf.keras.layers.Dense(256, activation='relu')\n",
    "#         self.fc3 = tf.keras.layers.Dense(128, activation='relu')\n",
    "#         self.output_layer = tf.keras.layers.Dense(num_actions, activation=None)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x = self.fc1(inputs)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DQN model\n",
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(num_actions, activation=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReplayBuffer:\n",
    "#     def __init__(self, buffer_size=10000):\n",
    "#         self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "#     def add(self, experience):\n",
    "#         self.buffer.append(experience)\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         return random.sample(self.buffer, batch_size)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size=10000):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define VRP Agent\n",
    "# class VRPAgent:\n",
    "#     def __init__(self, num_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, learning_rate=0.001):\n",
    "#         self.num_actions = num_actions\n",
    "#         self.gamma = gamma\n",
    "#         self.epsilon = epsilon\n",
    "#         self.epsilon_decay = epsilon_decay\n",
    "#         self.epsilon_min = epsilon_min\n",
    "#         self.model = DQN(num_actions)\n",
    "#         self.target_model = DQN(num_actions)\n",
    "#         self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "#         self.replay_buffer = ReplayBuffer()\n",
    "#         self.update_target_model()\n",
    "\n",
    "#     def update_target_model(self):\n",
    "#         self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "#     def choose_action(self, state, unvisited_nodes):\n",
    "#         state_tensor = tf.convert_to_tensor([[state]], dtype=tf.float32)\n",
    "#         if np.random.rand() < self.epsilon:\n",
    "#             return np.random.choice(list(unvisited_nodes))\n",
    "#         else:\n",
    "#             q_values = self.model(state_tensor).numpy()[0]\n",
    "#             masked_q_values = np.full_like(q_values, -np.inf)\n",
    "#             masked_q_values[list(unvisited_nodes)] = q_values[list(unvisited_nodes)]\n",
    "#             return np.argmax(masked_q_values)\n",
    "\n",
    "#     def store_experience(self, state, action, reward, next_state, done):\n",
    "#         state = np.atleast_2d(state)\n",
    "#         next_state = np.atleast_2d(next_state)\n",
    "#         self.replay_buffer.add((state, action, reward, next_state, done))\n",
    "\n",
    "#     def train(self, batch_size=32):\n",
    "#         if len(self.replay_buffer) < batch_size:\n",
    "#             return\n",
    "#         batch = self.replay_buffer.sample(batch_size)\n",
    "#         states, actions, rewards, next_states, dones = zip(*batch)\n",
    "#         states = tf.convert_to_tensor(np.array(states).squeeze(axis=1), dtype=tf.float32)\n",
    "#         next_states = tf.convert_to_tensor(np.array(next_states).squeeze(axis=1), dtype=tf.float32)\n",
    "#         rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "#         actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "#         dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "#         next_q_values = self.target_model(next_states)\n",
    "#         max_next_q_values = tf.reduce_max(next_q_values, axis=1)\n",
    "#         targets = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             q_values = self.model(states)\n",
    "#             action_masks = tf.one_hot(actions, self.num_actions)\n",
    "#             q_values = tf.reduce_sum(action_masks * q_values, axis=1)\n",
    "#             loss = tf.keras.losses.MSE(targets, q_values)\n",
    "#         gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "#         self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "#         if self.epsilon > self.epsilon_min:\n",
    "#             self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VRP Agent\n",
    "class VRPAgent:\n",
    "    def __init__(self, num_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, learning_rate=0.001):\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.model = DQN(num_actions)\n",
    "        self.target_model = DQN(num_actions)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state_tensor = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.choice(range(self.num_actions))\n",
    "        else:\n",
    "            q_values = self.model(state_tensor).numpy()[0]\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        state = np.atleast_2d(state)\n",
    "        next_state = np.atleast_2d(next_state)\n",
    "        self.replay_buffer.add((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self, batch_size=32):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "        batch = self.replay_buffer.sample(batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = tf.convert_to_tensor(np.array(states).squeeze(axis=1), dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(np.array(next_states).squeeze(axis=1), dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "        next_q_values = self.target_model(next_states)\n",
    "        max_next_q_values = tf.reduce_max(next_q_values, axis=1)\n",
    "        targets = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.model(states)\n",
    "            action_masks = tf.one_hot(actions, self.num_actions)\n",
    "            q_values = tf.reduce_sum(action_masks * q_values, axis=1)\n",
    "            loss = tf.keras.losses.MSE(targets, q_values)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_vrp_agent(agent, num_vehicles, capacity, customer_demands, distance_matrix, num_episodes=150, batch_size=32, update_target_every=10):\n",
    "#     best_routes = []\n",
    "#     best_total_distance = float('inf')\n",
    "\n",
    "#     for episode in range(num_episodes):\n",
    "#         total_distance = 0\n",
    "#         cumulative_reward = 0\n",
    "#         episode_routes = []\n",
    "#         unvisited_nodes = set(range(1, len(customer_demands) + 1))  # Adjusted for valid_nodes (exclude depot)\n",
    "\n",
    "#         for vehicle_idx in range(num_vehicles):\n",
    "#             route = [0]  # Start from depot (node index 0)\n",
    "#             load = 0\n",
    "#             current_node = 0\n",
    "\n",
    "#             while unvisited_nodes:\n",
    "#                 action = agent.choose_action(current_node, unvisited_nodes)\n",
    "\n",
    "#                 if action in unvisited_nodes:\n",
    "#                     dist = distance_matrix[current_node][action]\n",
    "#                     if load + customer_demands[action - 1] <= capacity:\n",
    "#                         route.append(action)\n",
    "#                         load += customer_demands[action - 1]\n",
    "#                         total_distance += dist\n",
    "#                         unvisited_nodes.remove(action)\n",
    "\n",
    "#                         reward = max(10 / (dist + 1e-5), 1)\n",
    "#                         reward += 10\n",
    "\n",
    "#                         cumulative_reward += reward\n",
    "#                         agent.store_experience(np.array([current_node]), action, reward, np.array([action]), False)\n",
    "\n",
    "#                         current_node = action\n",
    "#                     else:\n",
    "#                         reward = -100\n",
    "#                         agent.store_experience(np.array([current_node]), action, reward, np.array([current_node]), True)\n",
    "#                         break\n",
    "#                 else:\n",
    "#                     reward = -50\n",
    "#                     agent.store_experience(np.array([current_node]), action, reward, np.array([current_node]), True)\n",
    "#                     break\n",
    "\n",
    "#             if unvisited_nodes or vehicle_idx < num_vehicles - 1:\n",
    "#                 route.append(0)\n",
    "#                 total_distance += distance_matrix[current_node][0]\n",
    "#                 reward = 100\n",
    "#                 cumulative_reward += reward\n",
    "#             elif vehicle_idx == num_vehicles - 1:\n",
    "#                 print(f\"Last route ends at node {current_node} without returning to depot.\")\n",
    "\n",
    "#             episode_routes.append(route)\n",
    "\n",
    "#         if total_distance < best_total_distance:\n",
    "#             best_total_distance = total_distance\n",
    "#             best_routes = episode_routes\n",
    "\n",
    "#         print(f\"Episode {episode + 1}/{num_episodes} - Total Distance: {total_distance} - Reward: {cumulative_reward:.2f} - Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "#         agent.train(batch_size=batch_size)\n",
    "#         if episode % update_target_every == 0:\n",
    "#             agent.update_target_model()\n",
    "\n",
    "#     print(f\"Best Total Distance: {best_total_distance}\")\n",
    "#     return best_routes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Chosen action = 0, Reward = -10.0\n",
      "Episode 2: Chosen action = 0, Reward = -10.0\n",
      "Episode 3: Chosen action = 0, Reward = -10.0\n",
      "Episode 4: Chosen action = 1, Reward = -20.0\n",
      "Episode 5: Chosen action = 0, Reward = -10.0\n",
      "Episode 6: Chosen action = 0, Reward = -10.0\n",
      "Episode 7: Chosen action = 1, Reward = -20.0\n",
      "Episode 8: Chosen action = 0, Reward = -10.0\n",
      "Episode 9: Chosen action = 0, Reward = -10.0\n",
      "Episode 10: Chosen action = 1, Reward = -20.0\n",
      "Episode 11: Chosen action = 0, Reward = -10.0\n",
      "Episode 12: Chosen action = 1, Reward = -20.0\n",
      "Episode 13: Chosen action = 1, Reward = -20.0\n",
      "Episode 14: Chosen action = 0, Reward = -10.0\n",
      "Episode 15: Chosen action = 1, Reward = -20.0\n",
      "Episode 16: Chosen action = 0, Reward = -10.0\n",
      "Episode 17: Chosen action = 0, Reward = -10.0\n",
      "Episode 18: Chosen action = 0, Reward = -10.0\n",
      "Episode 19: Chosen action = 1, Reward = -20.0\n",
      "Episode 20: Chosen action = 0, Reward = -10.0\n",
      "Episode 21: Chosen action = 1, Reward = -20.0\n",
      "Episode 22: Chosen action = 1, Reward = -20.0\n",
      "Episode 23: Chosen action = 0, Reward = -10.0\n",
      "Episode 24: Chosen action = 0, Reward = -10.0\n",
      "Episode 25: Chosen action = 0, Reward = -10.0\n",
      "Episode 26: Chosen action = 1, Reward = -20.0\n",
      "Episode 27: Chosen action = 0, Reward = -10.0\n",
      "Episode 28: Chosen action = 1, Reward = -20.0\n",
      "Episode 29: Chosen action = 0, Reward = -10.0\n",
      "Episode 30: Chosen action = 0, Reward = -10.0\n",
      "Episode 31: Chosen action = 1, Reward = -20.0\n",
      "Episode 32: Chosen action = 0, Reward = -10.0\n",
      "Episode 33: Chosen action = 0, Reward = -10.0\n",
      "Episode 34: Chosen action = 0, Reward = -10.0\n",
      "Episode 35: Chosen action = 0, Reward = -10.0\n",
      "Episode 36: Chosen action = 0, Reward = -10.0\n",
      "Episode 37: Chosen action = 0, Reward = -10.0\n",
      "Episode 38: Chosen action = 0, Reward = -10.0\n",
      "Episode 39: Chosen action = 1, Reward = -20.0\n",
      "Episode 40: Chosen action = 1, Reward = -20.0\n",
      "Episode 41: Chosen action = 1, Reward = -20.0\n",
      "Episode 42: Chosen action = 1, Reward = -20.0\n",
      "Episode 43: Chosen action = 0, Reward = -10.0\n",
      "Episode 44: Chosen action = 0, Reward = -10.0\n",
      "Episode 45: Chosen action = 1, Reward = -20.0\n",
      "Episode 46: Chosen action = 0, Reward = -10.0\n",
      "Episode 47: Chosen action = 1, Reward = -20.0\n",
      "Episode 48: Chosen action = 0, Reward = -10.0\n",
      "Episode 49: Chosen action = 1, Reward = -20.0\n",
      "Episode 50: Chosen action = 0, Reward = -10.0\n",
      "Episode 51: Chosen action = 1, Reward = -20.0\n",
      "Episode 52: Chosen action = 1, Reward = -20.0\n",
      "Episode 53: Chosen action = 0, Reward = -10.0\n",
      "Episode 54: Chosen action = 1, Reward = -20.0\n",
      "Episode 55: Chosen action = 0, Reward = -10.0\n",
      "Episode 56: Chosen action = 0, Reward = -10.0\n",
      "Episode 57: Chosen action = 0, Reward = -10.0\n",
      "Episode 58: Chosen action = 1, Reward = -20.0\n",
      "Episode 59: Chosen action = 0, Reward = -10.0\n",
      "Episode 60: Chosen action = 1, Reward = -20.0\n",
      "Episode 61: Chosen action = 0, Reward = -10.0\n",
      "Episode 62: Chosen action = 0, Reward = -10.0\n",
      "Episode 63: Chosen action = 0, Reward = -10.0\n",
      "Episode 64: Chosen action = 0, Reward = -10.0\n",
      "Episode 65: Chosen action = 1, Reward = -20.0\n",
      "Episode 66: Chosen action = 1, Reward = -20.0\n",
      "Episode 67: Chosen action = 1, Reward = -20.0\n",
      "Episode 68: Chosen action = 0, Reward = -10.0\n",
      "Episode 69: Chosen action = 1, Reward = -20.0\n",
      "Episode 70: Chosen action = 1, Reward = -20.0\n",
      "Episode 71: Chosen action = 0, Reward = -10.0\n",
      "Episode 72: Chosen action = 0, Reward = -10.0\n",
      "Episode 73: Chosen action = 0, Reward = -10.0\n",
      "Episode 74: Chosen action = 0, Reward = -10.0\n",
      "Episode 75: Chosen action = 1, Reward = -20.0\n",
      "Episode 76: Chosen action = 0, Reward = -10.0\n",
      "Episode 77: Chosen action = 1, Reward = -20.0\n",
      "Episode 78: Chosen action = 1, Reward = -20.0\n",
      "Episode 79: Chosen action = 0, Reward = -10.0\n",
      "Episode 80: Chosen action = 0, Reward = -10.0\n",
      "Episode 81: Chosen action = 0, Reward = -10.0\n",
      "Episode 82: Chosen action = 0, Reward = -10.0\n",
      "Episode 83: Chosen action = 1, Reward = -20.0\n",
      "Episode 84: Chosen action = 0, Reward = -10.0\n",
      "Episode 85: Chosen action = 0, Reward = -10.0\n",
      "Episode 86: Chosen action = 1, Reward = -20.0\n",
      "Episode 87: Chosen action = 0, Reward = -10.0\n",
      "Episode 88: Chosen action = 1, Reward = -20.0\n",
      "Episode 89: Chosen action = 1, Reward = -20.0\n",
      "Episode 90: Chosen action = 0, Reward = -10.0\n",
      "Episode 91: Chosen action = 1, Reward = -20.0\n",
      "Episode 92: Chosen action = 1, Reward = -20.0\n",
      "Episode 93: Chosen action = 0, Reward = -10.0\n",
      "Episode 94: Chosen action = 0, Reward = -10.0\n",
      "Episode 95: Chosen action = 0, Reward = -10.0\n",
      "Episode 96: Chosen action = 1, Reward = -20.0\n",
      "Episode 97: Chosen action = 0, Reward = -10.0\n",
      "Episode 98: Chosen action = 0, Reward = -10.0\n",
      "Episode 99: Chosen action = 0, Reward = -10.0\n",
      "Episode 100: Chosen action = 0, Reward = -10.0\n",
      "The best route is 'lancar' with time 10.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Train the agent to choose between routes\n",
    "agent = VRPAgent(num_actions=2)\n",
    "state = np.array([route[\"Time\"] for route in routes_data], dtype=np.float32)  # Use times from routes\n",
    "num_episodes = 100\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    action = agent.choose_action(state)\n",
    "    reward = -state[action]  # Minimize time\n",
    "    next_state = state  # State remains constant\n",
    "    agent.store_experience(state, action, reward, next_state, True)\n",
    "    agent.train(batch_size=32)\n",
    "    print(f\"Episode {episode + 1}: Chosen action = {action}, Reward = {reward}\")\n",
    "\n",
    "# Get the best route\n",
    "best_action = agent.choose_action(state)\n",
    "print(f\"The best route is '{mapped_routes_with_paths[best_action]['Condition']}' with time {state[best_action]} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize and train the VRP Agent\n",
    "# capacity = 100\n",
    "# num_vehicles = 1\n",
    "# # Initialize and train the VRP Agent\n",
    "# num_actions = len(distance_matrix[0])\n",
    "# agent = VRPAgent(num_actions=num_actions)\n",
    "# routes = train_vrp_agent(agent, num_vehicles=1, capacity=100, customer_demands=valid_demands, distance_matrix=distance_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routes visualized in 'vrp_routes_with_network.html'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bd/1gfn4wvd2yj_rjzsdjyq1g940000gn/T/ipykernel_60688/2868313022.py:20: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  color_palette = cm.get_cmap('rainbow', len(mapped_routes_with_paths))\n"
     ]
    }
   ],
   "source": [
    "# Visualization with folium\n",
    "start_location = routes_data[0][\"Route\"][0]\n",
    "m = folium.Map(location=start_location, zoom_start=14)\n",
    "\n",
    "# Add start and end markers\n",
    "folium.Marker(\n",
    "    location=start_location,\n",
    "    popup=\"Start: Stasiun Gubeng\",\n",
    "    icon=folium.Icon(color=\"green\")\n",
    ").add_to(m)\n",
    "\n",
    "end_location = routes_data[0][\"Route\"][-1]\n",
    "folium.Marker(\n",
    "    location=end_location,\n",
    "    popup=\"End: Tunjungan plaza\",\n",
    "    icon=folium.Icon(color=\"red\")\n",
    ").add_to(m)\n",
    "\n",
    "# Color palette for routes\n",
    "color_palette = cm.get_cmap('rainbow', len(mapped_routes_with_paths))\n",
    "\n",
    "# Add routes to the map\n",
    "for i, route in enumerate(mapped_routes_with_paths):\n",
    "    route_color = mcolors.to_hex(color_palette(i / len(mapped_routes_with_paths)))\n",
    "    folium.PolyLine(\n",
    "        locations=route[\"Path\"],\n",
    "        color=route_color,\n",
    "        weight=5,\n",
    "        opacity=0.8,\n",
    "        tooltip=f\"Condition: {route['Condition']} | Time: {route['Time']} mins | Distance: {route['Distance']} km\"\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save the map\n",
    "m.save(\"vrp_routes_tunjungank.html\")\n",
    "print(\"Routes visualized in 'vrp_routes_with_network.html'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
